{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Final_SafetyWarningSystem.ipynb","provenance":[{"file_id":"14jg2Je43uMfVi_LAQPoBz_E72WeLmIOL","timestamp":1607876492833},{"file_id":"1rKzOhpJ_qHz3nea6lqLqNtkNUFDSTQLS","timestamp":1607818435563}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.6 64-bit","metadata":{"interpreter":{"hash":"887023df6ed94f5ff65b09d1402ed6091772b9fdf84cc2f68a0f74f72846f6fa"}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NTNeBqC74mGd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607876951016,"user_tz":300,"elapsed":22201,"user":{"displayName":"Preeti Kannapan","photoUrl":"","userId":"18180777828780170859"}},"outputId":"cc09d34e-81a3-4cdb-ccf5-294f998d81d5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQ8BOJ4R11GU","executionInfo":{"status":"ok","timestamp":1607876951224,"user_tz":300,"elapsed":22396,"user":{"displayName":"Preeti Kannapan","photoUrl":"","userId":"18180777828780170859"}},"outputId":"21a47ca0-3f41-44a5-e407-bb389541d1a6"},"source":["%cd /content/drive/Shareddrives/EECS504_Computer_Vision_Project/ObjectTracking"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/Shareddrives/EECS504_Computer_Vision_Project/ObjectTracking\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dUKCN0Cu4zFM"},"source":["import os\n","import json\n","import cv2\n","import math\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","import tensorflow as tf\n","from yolov3.utils import Load_Yolo_model, image_preprocess, postprocess_boxes, nms, draw_bbox, read_class_names\n","from yolov3.configs import *\n","import time\n","from google.colab.patches import cv2_imshow\n","\n","from deep_sort import nn_matching\n","from deep_sort.detection import Detection\n","from deep_sort.tracker import Tracker\n","from deep_sort import generate_detections as gdet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JILSiks7bxCP"},"source":["YOLO Object Tracking Function\n"]},{"cell_type":"code","metadata":{"id":"YFdn6qq9bwiW"},"source":["def Object_tracking(Yolo, video_path, output_path, input_size=416, show=False, CLASSES=YOLO_COCO_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors='', Track_only = []):\n","    # Definition of the parameters\n","    max_cosine_distance = 0.7\n","    nn_budget = None\n","    \n","    #initialize deep sort object\n","    model_filename = 'model_data/mars-small128.pb'\n","    encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n","    metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n","    tracker = Tracker(metric)\n","\n","    times, times_2 = [], []\n","\n","    if video_path:\n","        vid = cv2.VideoCapture(video_path) # detect on video\n","    else:\n","        vid = cv2.VideoCapture(0) # detect from webcam\n","\n","    # by default VideoCapture returns float instead of int\n","    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(vid.get(cv2.CAP_PROP_FPS))\n","    codec = cv2.VideoWriter_fourcc(*'XVID')\n","    out = cv2.VideoWriter(output_path, codec, fps, (width, height)) # output_path must be .mp4\n","\n","    NUM_CLASS = read_class_names(CLASSES)\n","    key_list = list(NUM_CLASS.keys()) \n","    val_list = list(NUM_CLASS.values())\n","    \n","    all_tracked_bboxes = []\n","    fps = []\n","\n","    while True:\n","        _, frame = vid.read()\n","\n","        try:\n","            original_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            original_frame = cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB)\n","        except:\n","            break\n","        \n","        image_data = image_preprocess(np.copy(original_frame), [input_size, input_size])\n","        image_data = image_data[np.newaxis, ...].astype(np.float32)\n","\n","        t1 = time.time()\n","        if YOLO_FRAMEWORK == \"tf\":\n","            pred_bbox = Yolo.predict(image_data)\n","        elif YOLO_FRAMEWORK == \"trt\":\n","            batched_input = tf.constant(image_data)\n","            result = Yolo(batched_input)\n","            pred_bbox = []\n","            for key, value in result.items():\n","                value = value.numpy()\n","                pred_bbox.append(value)\n","        \n","        t2 = time.time()\n","        \n","        pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n","        pred_bbox = tf.concat(pred_bbox, axis=0)\n","\n","        bboxes = postprocess_boxes(pred_bbox, original_frame, input_size, score_threshold)\n","        bboxes = nms(bboxes, iou_threshold, method='nms')\n","\n","        # extract bboxes to boxes (x, y, width, height), scores and names\n","        boxes, scores, names = [], [], []\n","        for bbox in bboxes:\n","            if len(Track_only) !=0 and NUM_CLASS[int(bbox[5])] in Track_only or len(Track_only) == 0:\n","                boxes.append([bbox[0].astype(int), bbox[1].astype(int), bbox[2].astype(int)-bbox[0].astype(int), bbox[3].astype(int)-bbox[1].astype(int)])\n","                scores.append(bbox[4])\n","                names.append(NUM_CLASS[int(bbox[5])])\n","\n","        # Obtain all the detections for the given frame.\n","        boxes = np.array(boxes) \n","        names = np.array(names)\n","        scores = np.array(scores)\n","        features = np.array(encoder(original_frame, boxes))\n","        detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(boxes, scores, names, features)]\n","\n","        # Pass detections to the deepsort object and obtain the track information.\n","        tracker.predict()\n","        tracker.update(detections)\n","\n","        # Obtain info from the tracks\n","        tracked_bboxes = []\n","        for track in tracker.tracks:\n","            if not track.is_confirmed() or track.time_since_update > 5:\n","                continue \n","            bbox = track.to_tlbr() # Get the corrected/predicted bounding box\n","            class_name = track.get_class() #Get the class name of particular object\n","            tracking_id = track.track_id # Get the ID for the particular track\n","            index = key_list[val_list.index(class_name)] # Get predicted object index by object name\n","            tracked_bboxes.append(bbox.tolist() + [tracking_id, index]) # Structure data, that we could use it with our draw_bbox function\n","        \n","        # Save all tracked_bboxes\n","        all_tracked_bboxes.append(tracked_bboxes)\n","\n","        # draw detection on frame\n","        image = draw_bbox(original_frame, tracked_bboxes, CLASSES=CLASSES, tracking=True)\n","\n","        t3 = time.time()\n","        times.append(t2-t1)\n","        times_2.append(t3-t1)\n","        \n","        times = times[-20:]\n","        times_2 = times_2[-20:]\n","\n","        ms = sum(times)/len(times)*1000\n","        fps.append(1000/ms)\n","        fps2 = 1000 / (sum(times_2)/len(times_2)*1000)\n","        \n","        # image = cv2.putText(image, \"Time: {:.1f} FPS\".format(fps), (0, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n","\n","        # draw original yolo detection\n","        #image = draw_bbox(image, bboxes, CLASSES=CLASSES, show_label=False, rectangle_colors=rectangle_colors, tracking=True)\n","\n","        # print(\"Time: {:.2f}ms, Detection FPS: {:.1f}, total FPS: {:.1f}\".format(ms, fps, fps2))\n","        if output_path != '': out.write(image)\n","        if show:\n","            cv2_imshow(image)\n","            \n","            if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n","                cv2.destroyAllWindows()\n","                break\n","            \n","    cv2.destroyAllWindows()\n","    return all_tracked_bboxes, fps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kkv-UiKCb-zE"},"source":["Command to Run YOLO Object Tracking"]},{"cell_type":"code","metadata":{"id":"uVthNiE0b-Gi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607877011895,"user_tz":300,"elapsed":83048,"user":{"displayName":"Preeti Kannapan","photoUrl":"","userId":"18180777828780170859"}},"outputId":"1836c7b8-fda3-49ce-a3ff-3409b96975ed"},"source":["yolo = Load_Yolo_model()\n","video_path   = \"./IMAGES/three_ppl_video_30fps_260frames.avi\"\n","all_tracked_bboxes, fps = Object_tracking(yolo, video_path, \"detection.mp4\", input_size=YOLO_INPUT_SIZE, show=False, iou_threshold=0.1, rectangle_colors=(255,0,0), Track_only = [\"person\"])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BKcWKB8Qm0iS"},"source":["Calculate BBox Centroids in Pixel Coords\r\n"]},{"cell_type":"code","metadata":{"id":"RWo3630Lmug3"},"source":["# Assign -1 to centroid coord when no bbox available\r\n","num_frames = 260\r\n","num_obj = 3\r\n","centroids_x = np.zeros((num_frames, num_obj)) \r\n","centroids_y = np.zeros((num_frames, num_obj))\r\n","centroids_x -= 1\r\n","centroids_y -= 1\r\n","for i in range(num_frames):\r\n","  tracked_bboxes = all_tracked_bboxes[i]\r\n","  num_bboxes = len(tracked_bboxes)\r\n","  for j in range(num_bboxes):\r\n","    obj_ID = tracked_bboxes[j][4]\r\n","    corner_xmin = tracked_bboxes[j][0]\r\n","    corner_ymin = tracked_bboxes[j][1]\r\n","    corner_xmax = tracked_bboxes[j][2]\r\n","    corner_ymax = tracked_bboxes[j][3]\r\n","    centroids_x[i, obj_ID-1] = (corner_xmin + corner_xmax)/2\r\n","    centroids_y[i, obj_ID-1] = (corner_ymin + corner_ymax)/2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MeCTfqENNWIU"},"source":["Load Camera Intrinsics, RGB and Depth Frame Arrays from JSON File. "]},{"cell_type":"code","metadata":{"id":"NZ8_d39a5XBj"},"source":["folder_path = \"./Princeton Datasets/three_people\"  ##Change the directory as needed\n","depth_images_path = folder_path + \"/depth\"\n","rgb_images_path = folder_path + \"/rgb\"\n","json_path = folder_path + \"/frames.json\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7u8tXf15bmq"},"source":["with open(json_path) as json_file:\n","    frames_json = json.load(json_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjfPSQn-JoYt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607877011927,"user_tz":300,"elapsed":83050,"user":{"displayName":"Preeti Kannapan","photoUrl":"","userId":"18180777828780170859"}},"outputId":"3a8b938d-f1b3-4c0d-9794-c29f5f02e39e"},"source":["num_frames = frames_json['length']\n","\n","K_matrix = frames_json['K']\n","cx = K_matrix[0][2]\n","cy = K_matrix[1][2]\n","fx = K_matrix[0][0]\n","fy = K_matrix[1][1]\n","\n","rgb_timestamps = frames_json['imageTimestamp']\n","depth_timestamps = frames_json['depthTimestamp']\n","\n","print(cx, cy, fx, fy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["320 240 575.8157496 575.8157496\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lXfaouDZ5b6n"},"source":["rgb_array = []\n","depth_array = []\n","\n","for frame_id in range(1,num_frames+1):\n","  rgb_image = rgb_images_path + '/r-{}-{}.png'.format(frames_json['imageTimestamp'][frame_id-1], frames_json['imageFrameID'][frame_id-1])\n","  rgb = cv2.imread(rgb_image)\n","  depth_image = depth_images_path + '/d-{}-{}.png'.format(frames_json['depthTimestamp'][frame_id-1], frames_json['depthFrameID'][frame_id-1])\n","  depth = cv2.imread(depth_image,-1)\n","  depth = np.bitwise_or(np.right_shift(depth,3), np.left_shift(depth,13))\n","  rgb_array.append(rgb)\n","  depth_array.append(depth)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFiIZQ05M_Bo"},"source":["Transform from Image Coords to Camera Coords"]},{"cell_type":"code","metadata":{"id":"c7toYGDgqy8b"},"source":["cam_coords_x = np.zeros((num_frames, num_obj))\r\n","cam_coords_y = np.zeros((num_frames, num_obj))\r\n","cam_coords_z = np.zeros((num_frames, num_obj))\r\n","cam_coords_x += 1000\r\n","cam_coords_y += 1000\r\n","cam_coords_z += 1000\r\n","\r\n","for i in range(num_frames):\r\n","  for j in range(num_obj):\r\n","    u = int(centroids_y[i,j])\r\n","    v = int(centroids_x[i,j])\r\n","\r\n","    if (u != -1):\r\n","      z = depth_array[i][u,v]/1000            #in m\r\n","      cam_coords_x[i,j] = -(u-cx)*z*1/fx      #in m\r\n","      cam_coords_y[i,j] = (v-cy)*z*1/fy       #in m\r\n","      cam_coords_z[i,j] = z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pw9IVBkkNE7S"},"source":["Calculate Object Velocities"]},{"cell_type":"code","metadata":{"id":"Nim6a-_usT2a"},"source":["# If 1000 is encountered, retain same position, velocity as before\r\n","\r\n","fps_const = 30\r\n","\r\n","vel_x = np.zeros((num_frames, num_obj))\r\n","vel_y = np.zeros((num_frames, num_obj))\r\n","vel_z = np.zeros((num_frames, num_obj))\r\n","\r\n","for i in range(1, num_frames):\r\n","  for j in range(num_obj):\r\n","    if (cam_coords_x[i,j] != 1000.0 and cam_coords_x[i-1,j] != 1000.0): \r\n","      vel_x[i,j] = (cam_coords_x[i,j] - cam_coords_x[i-1,j])*fps_const   \r\n","      vel_y[i,j] = (cam_coords_y[i,j] - cam_coords_y[i-1,j])*fps_const                             \r\n","      vel_z[i,j] = (cam_coords_z[i,j] - cam_coords_z[i-1,j])*fps_const\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKd2mt2HMwnn"},"source":["Post-process Positions and Velocities"]},{"cell_type":"code","metadata":{"id":"WqLmU-xlMnY_"},"source":["# First bounding box is at frame index 2\r\n","cam_coords_x[0,:] = cam_coords_x[2,:]\r\n","cam_coords_y[0,:] = cam_coords_y[2,:]\r\n","cam_coords_z[0,:] = cam_coords_z[2,:]\r\n","for i in range(1,num_frames):\r\n","  for j in range(num_obj):\r\n","    if cam_coords_x[i,j] == 1000.0: # no bbox for obj j in frame i \r\n","      cam_coords_x[i,j] = cam_coords_x[i-1,j]\r\n","      cam_coords_y[i,j] = cam_coords_y[i-1,j]\r\n","      cam_coords_z[i,j] = cam_coords_z[i-1,j]\r\n","      vel_x[i,j] = vel_x[i-1,j]\r\n","      vel_y[i,j] = vel_y[i-1,j]\r\n","      vel_z[i,j] = vel_z[i-1,j]\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DosYVwx1KZs-"},"source":["Warning System Function"]},{"cell_type":"code","metadata":{"id":"VoDT5wbUKZTp"},"source":["def warning_sys(frame_ind):\r\n","  i = frame_ind\r\n","  sys = []\r\n","  for j in range(0,2):\r\n","    for k in range(j+1, num_obj):\r\n","\r\n","      px1 = cam_coords_x[i,j]\r\n","      pz1 = cam_coords_z[i,j]\r\n","      vx1 = vel_x[i,j]\r\n","      vz1 = vel_z[i, j]\r\n","      v1 = np.array((vx1, vz1))\r\n","      \r\n","      px2 = cam_coords_x[i,k]\r\n","      pz2 = cam_coords_z[i,k]\r\n","      vx2 = vel_x[i,k]\r\n","      vz2 = vel_z[i,k]\r\n","      v2 = np.array((vx2, vz2))\r\n","\r\n","      r21 = np.array((vx2 - vx1, vz2 - vz1))  # relative vel of 2 wrt 1\r\n","      p12 = np.array((px1 - px2, pz1 - pz2))  # relative pos of 2 wrt 1\r\n","\r\n","      approaching_vel = np.dot(r21, p12)\r\n","      distance = np.sqrt(p12[0]**2 + p12[1]**2)\r\n","\r\n","      approaching_vel_tol = 0.5\r\n","      distance_tol = 0.3\r\n","\r\n","      far_fast = False\r\n","      close_slow = False # if included, errs more on the side of safety, many warnings\r\n","      close_fast = False\r\n","      # Far away, but approaching fast\r\n","      if (distance > distance_tol and approaching_vel > approaching_vel_tol):\r\n","        far_fast = True\r\n","      # Close by and approaching slow\r\n","      if (distance < distance_tol and approaching_vel > 0):\r\n","        close_slow = False\r\n","      # Close by and approaching fast\r\n","      if (distance < distance_tol and approaching_vel > approaching_vel_tol):\r\n","        close_fast = True\r\n","\r\n","      if (close_slow == True or close_fast == True):\r\n","        sys.append([\"Danger\", j+1, k+1])\r\n","      elif (far_fast == True):\r\n","        sys.append([\"Approaching\", j+1, k+1])\r\n","        \r\n","  return sys"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dy6F45NzNI6B"},"source":["Generate 2D figs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"AKt_xoq6BfXT","executionInfo":{"status":"ok","timestamp":1607877326224,"user_tz":300,"elapsed":397310,"user":{"displayName":"Preeti Kannapan","photoUrl":"","userId":"18180777828780170859"}},"outputId":"8effbb09-fbcf-4097-cd39-ead7e1027060"},"source":["props_y_coord = dict(boxstyle='round', facecolor='wheat', alpha=0.35)\n","for frame_num in range(num_frames):\n","  \n","  ax = plt.axes()\n","\n","  plt.text(0.77, 0.99, \"Y (in m)\", transform=ax.transAxes, fontsize=9,verticalalignment='top')\n","  \n","  for person in range(3):\n","    #PLOTTING ARROWS\n","    hL = 0.3\n","    hW = 0.2\n","    dx = vel_x[frame_num,person]\n","    dz = vel_z[frame_num,person]\n","    vec_ab_magnitude = math.sqrt(dx**2+dz**2)\n","    dx = dx / vec_ab_magnitude\n","    dz = dz / vec_ab_magnitude\n","    arrow_colors = ['blue', 'red', 'green']\n","    ax.arrow(cam_coords_x[frame_num,person],cam_coords_z[frame_num,person], dx,dz,head_width=hW, head_length=hL, fc=arrow_colors[person], ec=arrow_colors[person])\n","    #PLOTTING Y_COORDINATES\n","    plt.text(0.77, (0.94-person*0.05), 'Person '+ str(person+1)+': ' +  '{:.2f}'.format(-cam_coords_y[frame_num,person]), transform=ax.transAxes, fontsize=9,verticalalignment='top', color= arrow_colors[person])\n","  \n","  #SCATTER PLOT FOR POSITION OF PERSONS\n","  plt.scatter(cam_coords_x[frame_num,:],cam_coords_z[frame_num,:],color=arrow_colors)\n","\n","  #TITLE\n","  plt.title(\"SAFETY WARNING SYSTEM\")\n","\n","  #WARNINGS\n","  warn_list = warning_sys(frame_num)\n","  s1=\"\"\n","  s2=\"\"\n","  for warn in warn_list:\n","    if (warn[0] == 'Danger'):\n","      if (s1 == \"\"):\n","        s1 = \"DANGER: \"+ \" [\" + str(warn[1])+\", \"+str(warn[2]) +\"]\"\n","      else: \n","        s1 = s1+ \" [\" + str(warn[1])+\", \"+str(warn[2]) +\"]\"\n","      \n","    elif (warn[0] == 'Approaching'):\n","      if (s2==\"\"):\n","        s2 = \"Approaching: \"+ \" [\" + str(warn[1])+\", \"+str(warn[2]) +\"]\"\n","      else: \n","        s2 = s2+ \" [\" + str(warn[1])+\", \"+str(warn[2]) +\"]\"\n","\n","  props1 = dict(boxstyle='round', facecolor='red', alpha=0.4)\n","  props2 = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n","  if ((s1 != \"\") and (s2!=\"\")):\n","    plt.text(0.05, 0.95, s1, transform=ax.transAxes, fontsize=12,verticalalignment='top', bbox=props1)\n","    plt.text(0.05, 0.85, s2, transform=ax.transAxes, fontsize=12,verticalalignment='top', bbox=props2)\n","  elif (s1!=\"\"):\n","    plt.text(0.05, 0.95, s1, transform=ax.transAxes, fontsize=12,verticalalignment='top', bbox=props1)\n","  elif (s2!=\"\"):\n","    plt.text(0.05, 0.95, s2, transform=ax.transAxes, fontsize=12,verticalalignment='top', bbox=props2)\n","\n","  #AXIS LABELS\n","  plt.xlabel(\"X (in m)\")\n","  plt.ylabel(\"Depth (in m)\")\n","\n","  #AXIS LIMITS\n","  plt.xlim(-1.5, 1.5)\n","  plt.ylim(0,4.5)\n","  plt.grid()\n","\n","  #SAVE FIGURES\n","  plt.savefig('./2Dfigs/' + str(frame_num) + '.png')\n","  plt.clf()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n","  app.launch_new_instance()\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"LO9CbDvMvjwp"},"source":["Convert 2D figs to mp4 video"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ov9byl7Gvm7X","executionInfo":{"status":"ok","timestamp":1607877328016,"user_tz":300,"elapsed":399094,"user":{"displayName":"Preeti Kannapan","photoUrl":"","userId":"18180777828780170859"}},"outputId":"c3c22142-c0b1-4034-a238-b6aa4b09e2c0"},"source":["import os\n","from os.path import isfile, join\n","\n","pathIn= './2Dfigs'\n","pathOut = '2D_vid.mp4'\n","fps_const = 30\n","frame_array = []\n","files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n","#for sorting the file names properly\n","files.sort(key = lambda x: int((x.split(\".\")[0])))\n","total_frames = len(files)\n","num_files = 260\n","for i in range(num_files):\n","    filename = pathIn + '/' + files[i]\n","    #reading each files\n","    img = cv2.imread(filename) \n","    height, width, layers = img.shape\n","    size = (width,height)\n","    #inserting the frames into an image array\n","    frame_array.append(img)\n","print(\"Frame array loaded!\")\n","out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps_const, size)\n","print(\"out initialized!\")\n","for i in range(len(frame_array)):\n","    # writing to a image array\n","    out.write(frame_array[i])\n","out.release()\n","print(\"out loaded!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Frame array loaded!\n","out initialized!\n","out loaded!\n"],"name":"stdout"}]}]}